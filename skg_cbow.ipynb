{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "c4aaa09d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import nltk\n",
    "import re\n",
    "from bs4 import BeautifulSoup\n",
    "import numpy as np\n",
    "import gensim \n",
    "from nltk.tokenize import sent_tokenize, word_tokenize \n",
    "import warnings \n",
    "import logging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "2a6971ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hide all the warnings in jupyter notebook\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings ('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "f8d39c4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "df1 = pd.read_csv('AppReviews.csv',encoding='utf-8')\n",
    "df2 = pd.read_csv('JIRA.csv',encoding='utf-8')\n",
    "df3 = pd.read_csv('StackOverflow.csv',encoding='utf-8').drop('id', axis=1)\n",
    "df4 = pd. concat([df1, df2, df3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "ff39ab86",
   "metadata": {},
   "outputs": [],
   "source": [
    "df4['sentence'].to_csv('alice.txt', sep = \"\\n\", index = False)\n",
    "\n",
    "sample = open(\"alice.txt\", \"r\",encoding=\"utf8\") \n",
    "s = sample.read() \n",
    "f = s.replace(\"\\n\", \" \") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "333c6184",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = [] \n",
    "for i in sent_tokenize(f): \n",
    "\ttemp = [] \n",
    "\t\n",
    "\t# tokenize the sentence into words \n",
    "\tfor j in word_tokenize(i): \n",
    "\t\ttemp.append(j.lower()) \n",
    "\n",
    "\tdata.append(temp) \n",
    "model1 = gensim.models.Word2Vec(data, min_count = 1,vector_size=100, window = 5) \n",
    "#<---deprecated model1 = gensim.models.Word2Vec(data, min_count = 1,size=100, window = 5)-------->deprecated#\n",
    "\n",
    "model2 = gensim.models.Word2Vec(data, min_count = 1,vector_size=100,window = 5, sg = 1) \n",
    "\n",
    "#<---deprecated model2 = gensim.models.Word2Vec(data, min_count = 1,size=100,window = 5, sg = 1-------->deprecated#"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "8bb0e3ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "REPLACE_BY_SPACE_RE = re.compile('[/(){}\\[\\]\\|@,;]')\n",
    "BAD_SYMBOLS_RE = re.compile('[^0-9a-z #+_]')\n",
    "STOPWORDS =nltk.corpus.stopwords.words('english')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "712175bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_text(text):\n",
    "    if type(text)==str:\n",
    "        \n",
    "        text = BeautifulSoup(text, \"lxml\").text # HTML decoding\n",
    "        text = text.lower() # lowercase text\n",
    "        text = REPLACE_BY_SPACE_RE.sub(' ', text) # replace REPLACE_BY_SPACE_RE symbols by space in text\n",
    "        text = BAD_SYMBOLS_RE.sub('', text) # delete symbols which are in BAD_SYMBOLS_RE from text\n",
    "        text = ' '.join(word for word in text.split() if word not in STOPWORDS) # delete stopwors from text\n",
    "        return text\n",
    "    return \"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "d0586677",
   "metadata": {},
   "outputs": [],
   "source": [
    "def wordprint(words,model1):\n",
    "    all_words, mean = set(), []\n",
    "    for word in words:\n",
    "        if isinstance(word, np.ndarray):\n",
    "            mean.append(word)\n",
    "        elif word in model1.wv.key_to_index:\n",
    "            a=np.array(model1.wv[word])\n",
    "            mean.append(a)\n",
    "    if not mean:\n",
    "        # FIXME: remove these examples in pre-processing\n",
    "        return np.zeros(100,)   \n",
    "    mean = gensim.matutils.unitvec(np.array(mean).mean(axis=0)).astype(np.float32)\n",
    "    return mean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "919c3f5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def  word_averaging_listn(wv, text_list):\n",
    "    return np.vstack([wordprint(post,wv) for post in text_list ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "b2d88d55",
   "metadata": {},
   "outputs": [],
   "source": [
    "def w2v_tokenize_text(text):\n",
    "    tokens = []\n",
    "    for sent in nltk.sent_tokenize(text, language='english'):\n",
    "        for word in nltk.word_tokenize(sent, language='english'):\n",
    "            if len(word) < 2:\n",
    "                continue\n",
    "            tokens.append(word)\n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "f4d206f5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                            sentence  oracle\n",
      "0  package file invalid phone factory reset wante...      -1\n",
      "1  iffy nice clean app sometimes works times does...      -1\n",
      "2                             cool freezes everytime      -1\n",
      "3  network error suddenly downloading update pack...      -1\n",
      "4  annoying let choose pictures want freezes forc...      -1\n",
      "                                            sentence  oracle\n",
      "0                                        guys stupid      -1\n",
      "1  lost whole morning cause hbases regionserver d...      -1\n",
      "2                       quote messing deep hbase dfs      -1\n",
      "3                   think going sweep shit kill root      -1\n",
      "4  idiot yeah idiotpath good commonslang hairball...      -1\n",
      "    id                                           sentence  oracle\n",
      "0    6                                      sadly working      -1\n",
      "1   78  everything builds fine try deploy application ...      -1\n",
      "2   90                     causing null pointer exception      -1\n",
      "3  139            attempts ive made shortcut unsuccessful      -1\n",
      "4  162                                           dont use      -1\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "fn=['AppReviews','JIRA','StackOverflow']\n",
    "for i in range(0,3):\n",
    "    fname=fn[i]+'.csv'\n",
    "    df = pd.read_csv(fname,encoding='utf-8')\n",
    "    df['sentence'] = df['sentence'].apply(clean_text)\n",
    "    print(df.head())\n",
    "    comtdata=df\n",
    "    test_tokenized = comtdata.apply(lambda r: w2v_tokenize_text(r['sentence']), axis=1).values\n",
    "    X_comtdata_average1 = word_averaging_listn(model1,test_tokenized)\n",
    "    fname=fn[i]+'cbow.csv'\n",
    "    np.savetxt(fname,X_comtdata_average1, delimiter=',', fmt='%f')\n",
    "    X_comtdata_average1 = word_averaging_listn(model2,test_tokenized)\n",
    "    fname=fn[i]+'skg.csv'\n",
    "    np.savetxt(fname,X_comtdata_average1, delimiter=',', fmt='%f')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
